{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 2865,
     "status": "ok",
     "timestamp": 1597591768704,
     "user": {
      "displayName": "js h",
      "photoUrl": "",
      "userId": "07106812964232388257"
     },
     "user_tz": -540
    },
    "id": "akqGTvBg-Z4G"
   },
   "outputs": [],
   "source": [
    "reduction_ratio = 16\n",
    "epochs = 1200\n",
    "batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 5020,
     "status": "ok",
     "timestamp": 1597591770865,
     "user": {
      "displayName": "js h",
      "photoUrl": "",
      "userId": "07106812964232388257"
     },
     "user_tz": -540
    },
    "id": "BU5SDXOw-Z4I"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4977,
     "status": "ok",
     "timestamp": 1597591770876,
     "user": {
      "displayName": "js h",
      "photoUrl": "",
      "userId": "07106812964232388257"
     },
     "user_tz": -540
    },
    "id": "7BGQxb8a-Z4S",
    "outputId": "9476bcdd-513c-4cd3-8c84-72d65b73efb1"
   },
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=10)\n",
    "splited = kfold.split(train, train['digit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['digit']\n",
    "del train['digit']\n",
    "del train['id']\n",
    "del train['letter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = [str(i) for i in range(784)]\n",
    "total_image_df = train[col_list]\n",
    "test_image_df = test[col_list]\n",
    "\n",
    "total_arr = total_image_df.values.reshape((len(total_image_df), 28, 28))\n",
    "test_arr = test_image_df.values.reshape((len(test_image_df), 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 8305,
     "status": "ok",
     "timestamp": 1597591774262,
     "user": {
      "displayName": "js h",
      "photoUrl": "",
      "userId": "07106812964232388257"
     },
     "user_tz": -540
    },
    "id": "rFunAXeV-Z4q"
   },
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255.,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.3,\n",
    "    height_shift_range=0.3,\n",
    "    shear_range=0.3,\n",
    "    zoom_range=0.3,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SE-ResNet\n",
    "class SE_ResidualUnit(tf.keras.layers.Layer):  \n",
    "    def __init__(self, filter_in, filter_out, reduction_ratio, kernel_size,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ##HyperParameter##\n",
    "        self.filter_in = filter_in\n",
    "        self.filter_out = filter_out\n",
    "        self.reduction_ratio = reduction_ratio\n",
    "        self.kernel_size = kernel_size\n",
    "        ##################\n",
    "        \n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.elu1 = tf.keras.layers.ELU()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filter_out//2, (1, 1),  kernel_initializer=\"he_normal\", padding='same', kernel_constraint=tf.keras.constraints.max_norm(1., axis=[0,1]))\n",
    "\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.elu2 = tf.keras.layers.ELU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filter_out//2, kernel_size, kernel_initializer=\"he_normal\",\n",
    "            padding = 'same', kernel_constraint=tf.keras.constraints.max_norm(1., axis=[0,1]))\n",
    "\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.elu3 = tf.keras.layers.ELU()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filter_out, (1, 1), kernel_initializer=\"he_normal\", padding='same',\n",
    "             kernel_constraint=tf.keras.constraints.max_norm(1., axis=[0,1]))\n",
    "        \n",
    "        self.gp = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.dense1 = tf.keras.layers.Dense(\n",
    "            filter_out//reduction_ratio, kernel_initializer=\"he_normal\", activation='elu' ,use_bias=False)\n",
    "        self.dense2 = tf.keras.layers.Dense(\n",
    "            filter_out, activation='sigmoid', kernel_initializer=\"he_normal\", use_bias=False)\n",
    "        self.reshape = tf.keras.layers.Reshape([1, 1, filter_out])\n",
    "        self.mul = tf.keras.layers.Multiply()\n",
    "        if filter_in == filter_out:\n",
    "          self.identity = lambda x:x\n",
    "        else:\n",
    "          self.identity = tf.keras.layers.Conv2D(\n",
    "                filter_out, (1, 1),  padding='same')\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = self.elu1(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        h = self.bn2(h, training=training)\n",
    "        h = self.elu2(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        h = self.bn3(h, training=training)\n",
    "        h = self.elu3(h)\n",
    "        h = self.conv3(h)\n",
    "\n",
    "        s = self.gp(h)\n",
    "        s = self.dense1(s)\n",
    "        s = self.dense2(s)\n",
    "        s = self.reshape(s)\n",
    "        s = self.mul([s, h])\n",
    "        return self.identity(x) + s\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\"filter_in\":self.filter_in, \"filter_out\":self.filter_out, \n",
    "                       \"reduction_ratio\":self.reduction_ratio,\"kernel_size\":self.kernel_size\n",
    "                       })\n",
    "        return config\n",
    "\n",
    "class ResidualUnit(tf.keras.layers.Layer):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        ##HyperParameter##\n",
    "        self.filter_in = filter_in\n",
    "        self.filter_out = filter_out\n",
    "        self.kernel_size = kernel_size\n",
    "        ##################\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.elu1 = tf.keras.layers.ELU()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filter_out//2, (1, 1), padding='same', kernel_initializer=\"he_normal\", kernel_constraint=tf.keras.constraints.max_norm(1., axis=[0,1]))\n",
    "\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.elu2 = tf.keras.layers.ELU()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filter_out//2, kernel_size, padding='same', kernel_initializer=\"he_normal\", kernel_constraint=tf.keras.constraints.max_norm(1., axis=[0,1]))\n",
    "\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization()\n",
    "        self.elu3 = tf.keras.layers.ELU()\n",
    "        self.conv3 = tf.keras.layers.Conv2D(\n",
    "            filter_out, (1, 1), padding='same', kernel_initializer=\"he_normal\", kernel_constraint=tf.keras.constraints.max_norm(1., axis=[0,1]))\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "          self.identity = lambda x:x\n",
    "        else:\n",
    "          self.identity = tf.keras.layers.Conv2D(\n",
    "                filter_out, (1, 1),  padding='same')\n",
    "\n",
    "    def call(self, x, training=None):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = self.elu1(h)\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        h = self.bn2(h, training=training)\n",
    "        h = self.elu2(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        h = self.bn3(h, training=training)\n",
    "        h = self.elu3(h)\n",
    "        h = self.conv3(h)\n",
    "\n",
    "        return self.identity(x) + h\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({ \"filter_in\":self.filter_in, \"filter_out\":self.filter_out, \"kernel_size\":self.kernel_size\n",
    "                        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##SE_ResNet##\n",
    "def build_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), kernel_initializer='he_normal', activation='elu', padding='same',\n",
    "                                    kernel_constraint=tf.keras.constraints.max_norm(1., axis=[0,1]), input_shape=[28,28,1]))\n",
    "\n",
    "    prev_channel = 128\n",
    "    idx = 0\n",
    "    for channel in [128] * 4 + [256] * 4 + [512] * 1:\n",
    "        if idx == 3 or idx == 4:\n",
    "            model.add(SE_ResidualUnit(filter_in=prev_channel, filter_out=channel, reduction_ratio=reduction_ratio, kernel_size=(3,3)))\n",
    "            model.add(tf.keras.layers.MaxPool2D((2,2)))\n",
    "            model.add(tf.keras.layers.Dropout(0.2))   \n",
    "        else:\n",
    "            model.add(SE_ResidualUnit(filter_in=prev_channel, filter_out=channel, reduction_ratio=reduction_ratio, kernel_size=(3,3)))\n",
    "        idx += 1\n",
    "        prev_channel = channel\n",
    "\n",
    "    model.add(ResidualUnit(512, 512, (3,3)))\n",
    "    model.add(tf.keras.layers.MaxPool2D((3,3)))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1024, activation='elu', kernel_initializer='he_normal', kernel_constraint=tf.keras.constraints.max_norm(1.)))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax')) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========0th========= \n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 57 steps, validate for 6 steps\n",
      "Epoch 1/1200\n",
      "57/57 [==============================] - 24s 422ms/step - loss: 6.5966 - accuracy: 0.1149 - val_loss: 3.1850 - val_accuracy: 0.1406\n",
      "Epoch 2/1200\n",
      "57/57 [==============================] - 6s 102ms/step - loss: 3.0978 - accuracy: 0.1110 - val_loss: 5.7749 - val_accuracy: 0.1094\n",
      "Epoch 3/1200\n",
      "50/57 [=========================>....] - ETA: 0s - loss: 2.9804 - accuracy: 0.1374"
     ]
    }
   ],
   "source": [
    "for k, (train, val) in enumerate(splited):\n",
    "  print(\"=========={}th========= \".format(k))\n",
    "  model = build_model()\n",
    "  model.compile(loss='sparse_categorical_crossentropy', optimizer=tf.keras.optimizers.Nadam(epsilon=1e-04), metrics=[\"accuracy\"])\n",
    "  train_x, train_y = total_arr[train], target[train]\n",
    "  val_x, val_y = total_arr[val], target[val]\n",
    "  train_img = train_x[...,tf.newaxis]\n",
    "  val_img = val_x[...,tf.newaxis]\n",
    "  filename = \"emnist_{}.h5\".format(k)\n",
    "  checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(filename, save_best_only=True, monitor='val_loss', mode='auto')\n",
    "  earlystopping = tf.keras.callbacks.EarlyStopping(patience=100, restore_best_weights=True)\n",
    "  lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(factor=0.8, patience=30)\n",
    "  model.fit(train_datagen.flow(train_img, train_y, shuffle=True, batch_size=batch), epochs=epochs, validation_data=test_datagen.flow(val_img, val_y,batch_size=batch),\n",
    "                              steps_per_epoch=len(train_img)//batch, validation_steps=len(val_img)//batch, callbacks=[checkpoint_cb, earlystopping, lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img = test_arr[...,tf.newaxis] # add channel\n",
    "test_img = test_img/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_models():\n",
    "    models = []\n",
    "    for i in range(10):\n",
    "        filename = \"emnist_{}.h5\".format(i)\n",
    "        model = tf.keras.models.load_model(filename, custom_objects={\"SE_ResidualUnit\":SE_ResidualUnit, \"ResidualUnit\":ResidualUnit})\n",
    "        models.append(model)\n",
    "    return models\n",
    "\n",
    "def get_predict(models):\n",
    "    predicts = []\n",
    "    for i in range(10):\n",
    "        predict = models[i].predict(test_img)\n",
    "        predicts.append(predict)\n",
    "    return predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_models()\n",
    "predicts = get_predict(models)\n",
    "predict_mean = predicts[0]\n",
    "for i in range(1, len(predicts)):\n",
    "    predict_mean += predicts[i]\n",
    "predict_mean = predict_mean/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ukcgyZ5e-Z5E"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dZG2-Hs89Gp"
   },
   "outputs": [],
   "source": [
    "digit = np.argmax(predict_mean, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vhAGWb6w-Z5H"
   },
   "outputs": [],
   "source": [
    "submission.digit = digit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umOiaq5J-Z5K"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('final_sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
